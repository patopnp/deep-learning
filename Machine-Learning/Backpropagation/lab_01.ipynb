{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todas las funciones que tiene que implementar\n",
    "from solutions import sigmoid, sigmoid_jac, softmax, softmax_jac, MSE, MSE_grad, densa_forward, forward, get_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la RED\n",
    "Dados los pesos y la estructura de una red neuronal con una softmax a la salida y un MSE como Loss calcular todo lo que se pide a continuación (No es comun usar MSE con la softmax pero a fines didácticos simplifica. Queda como ejercicio adicional resolver el mismo ejercicio pero con una categorical crossentropy a la saluda)\n",
    "\n",
    "Las funciones de activación de las capas A1 y A2 son sigmoideas (Queda como ejercicio también probar con otras funciones de activación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![red.png](red.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "D1 (Dense)                   (None, 3)                 9         \n",
    "_________________________________________________________________\n",
    "A1 (Activation)              (None, 3)                 0         \n",
    "_________________________________________________________________\n",
    "D2 (Dense)                   (None, 2)                 8         \n",
    "_________________________________________________________________\n",
    "A2 (Activation)              (None, 2)                 0         \n",
    "_________________________________________________________________\n",
    "D3 (Dense)                   (None, 3)                 9         \n",
    "_________________________________________________________________\n",
    "P_est (Activation)           (None, 3)                 0         \n",
    "=================================================================\n",
    "Total params: 26\n",
    "Trainable params: 26\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pesos de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa Densa 1 ws: 2x3\n",
      "[[0.10820953 0.3432914  0.1744045 ]\n",
      " [0.05457611 0.54989725 0.34384015]]\n",
      "\n",
      "Capa Densa 1 biases:\n",
      "[-0.67943245 -0.00294854  0.15257952]\n",
      "\n",
      "Capa Densa 2 - ws: 3x3\n",
      "[[-0.7706185  -0.17550795]\n",
      " [-0.10197585  0.45046437]\n",
      " [ 0.00585397  0.3024927 ]]\n",
      "\n",
      "Capa Densa 2 - biases\n",
      "[-0.10661452 -0.34508756]\n",
      "\n",
      "Capa Densa 3 - ws: 2x3\n",
      "[[-0.49749678 -0.40208894 -0.85052264]\n",
      " [ 1.0619878   0.07141189  0.17314   ]]\n",
      "\n",
      "Capa Densa 3 - biases\n",
      "[-0.29359275 -0.7259881   0.578059  ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = np.load('weights_softmax_3_layers.npy', allow_pickle=True)\n",
    "capas = ['Capa Densa 1 ws: 2x3', 'Capa Densa 1 biases:', 'Capa Densa 2 - ws: 3x3', 'Capa Densa 2 - biases', 'Capa Densa 3 - ws: 2x3', 'Capa Densa 3 - biases']\n",
    "for i, layer in enumerate(weights):\n",
    "    print(capas[i])\n",
    "    print(layer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dimensión de entrada es 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizar el Forward de la RED y contestar las preguntas\n",
    "### Se usará el siguiente vector de entrada para resolver el ejercicio\n",
    "Nota importante: Todos los vectores de salida de las capas son vectores filas al igual que X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Vector de entrada de ejemplo\n",
    "X = np.array([[3.4, 2.1]])\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894142, 0.26894142])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1+np.exp(np.array([1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de funciones para resolver las preguntas:\n",
    "En el archivo solutions.py verá que estan todas las funciones sin completar. Completelas a medida que se vaya solicitando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de forward capa densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "[[-0.19691022  2.31902646  1.46761914]]\n"
     ]
    }
   ],
   "source": [
    "# implementar función densa_forward que reciba X, W, b - Entrada, pesos, bias\n",
    "# Y devuelva la salida a la capa densa\n",
    "# Es simplemente una multiplicación de matrices mas una suma\n",
    "# La encontrará en el archivo solutions.py\n",
    "D1_out = densa_forward(X, weights[0], weights[1])\n",
    "print(D1_out.shape)\n",
    "print(D1_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 1\n",
    "\n",
    "¿Cuanto vale el vector de salida de la primera capa densa (sin activación)?\n",
    "a) [[-0.19691022  2.31902646  1.46761914]]\n",
    "b) [[0.45093089 0.91044059 0.81269524]]\n",
    "c) [[-0.21660124  2.5509291   1.61438106]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de forward sigmoidea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45093089 0.91044059 0.81269524]]\n"
     ]
    }
   ],
   "source": [
    "# Implementar la función sigmoid que recibe un vector y devuelve un vector con la sigmoidea de cada componente\n",
    "# Recordar que no hace falta un ciclo for\n",
    "# La encontrará en el archivo solutions.py\n",
    "A1_out = sigmoid(D1_out)\n",
    "print(A1_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 2\n",
    " \n",
    "¿Cuanto vale el vector de salida de la primera capa densa (con activación)?\n",
    "a) [[-0.19691022  2.31902646  1.46761914]]\n",
    "b) [[0.45093089 0.91044059 0.81269524]]\n",
    "c) [[-0.21660124  2.5509291   1.61438106]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salida 3er capa densa para entrada X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11573172 -0.8340024   0.36189705]]\n"
     ]
    }
   ],
   "source": [
    "D2_out = densa_forward(A1_out, weights[2], weights[3])\n",
    "A2_out = sigmoid(D2_out)\n",
    "D3_out = densa_forward(A2_out, weights[4], weights[5])\n",
    "print(D3_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 3\n",
    "¿Cuanto vale el vector de salida de la tercer capa densa (antes de la softmax)?\n",
    "\n",
    "a) [[-0.19691022  2.31902646  1.46761914]]\n",
    "b) [[ 0.11573172 -0.8340024   0.36189705]]\n",
    "c) [[-0.21660124  2.5509291   1.61438106]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de forward softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37510012 0.14510518 0.4797947 ]]\n"
     ]
    }
   ],
   "source": [
    "# Implementar softmax, que recibe un vector de entrada y devuelve la softmax de la entrada (un vector con probabilidades que suman 1)\n",
    "P_est = softmax(D3_out)\n",
    "print(P_est)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 4\n",
    "¿Cuanto vale la salida de la softmax, es decir las probabilidades estimadas (P_est)?\n",
    "\n",
    "a) [[7.85900624e-02 5.89827171e-06 9.21404039e-01]]\n",
    "b) [[0.34079456 0.30991759 0.34928785]]\n",
    "c) [[0.37510012 0.14510518 0.4797947 ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación forward MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Vector de salida de ejemplo\n",
    "P_true = np.array([[1, 0, 0]])\n",
    "print(P_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2139194440951749"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementar función MSE\n",
    "# debe devolver el mean square error en funcion de las probabilidades estimadas y las ground truth. \n",
    "# Recuerde que la salida es un escalar por lo que tiene que promediar\n",
    "MSE(P_est, P_true)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 5\n",
    "¿Cuanto vale el MSE?\n",
    "\n",
    "a) 0.2139194440951749\n",
    "b) 2.3804526201624125\n",
    "c) 0.10897951486491775"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar la función forward(X, P_true, weights) que devuelva P_est, mse, X, A1_out, A2_out (Opcional)\n",
    "Simplemente colocar todo el procedimiento anterior en una única función. \n",
    "\n",
    "Esta función sería un equivalente a un predict_proba mas un evaluate de keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.37510012, 0.14510518, 0.4797947 ]]), 0.2139194440951749, array([[3.4, 2.1]]), array([[0.45093089, 0.91044059, 0.81269524]]), array([[0.36767696, 0.55767363]]))\n"
     ]
    }
   ],
   "source": [
    "out = forward(X, P_true, weights)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El print debería arrojar lo siguiente:  \n",
    "[[0.37510012 0.14510518 0.4797947 ]] 0.2139194440951749 [[3.4 2.1]] [[0.45093089 0.91044059 0.81269524]] [[0.36767696 0.55767363]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward de la RED (Backpropagation)\n",
    "\n",
    "Realizaremos el backward completo de la red hasta llegar a la capa D1. Notar que todos los gradientes y jacabianos que calculamos salvo el último serán respecto a la entrada de cada capa\n",
    "\n",
    "**Nota importante**: Siempre que calcule un gradiento o un jacobiano es importante que las dimensiones de las matrices sean (entrada X salida) para evitar tener que trasponer matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente de la función de costo MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[-0.41659992]\n",
      " [ 0.09673679]\n",
      " [ 0.31986314]]\n"
     ]
    }
   ],
   "source": [
    "# Implementar MSE_grad(P_true, P_est) que devuelva el gradiente del MSE evaluado en P_est respecto a cada una de las entradas del bloque \n",
    "# No olvidar dividir por 3 (Idealmente hagalo genérico)\n",
    "MSE_grad_out = MSE_grad(P_true, P_est)\n",
    "print(MSE_grad_out.shape)\n",
    "print(MSE_grad_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 6:\n",
    "¿Cuanto vale el gradiente del MSE evaluado en el P_est?\n",
    "\n",
    "a) [[-1.08326659  0.09673679  0.31986314]].T\n",
    "b) [[-0.41659992  0.09673679  0.31986314]].T\n",
    "c) [[-0.08326659  0.09673679  0.31986314]].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobiano de Softmax:\n",
    "\n",
    "Implementar una función que devuelva el jacobiano de la softmax evaluado en un vector fila (respecto a las entradas)\n",
    "\n",
    "Aca hay una pagina interesante: https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/\n",
    "\n",
    "Solo se necesita implementar esto:\n",
    "\n",
    "![softmax_jacobiano.png](softmax_jacobiano.png)\n",
    "\n",
    "Donde $\\sigma(x_1)$ es la salida de la primera componente de la sofmax, es decir, la primera matriz es el resultado de la softmax expresado como matriz diagonal y se puede lograr con np.diag(softmax(X).reshape(-1)). Mientras que la segunda matriz se puede implementar como el producto punto entre la salida de la softmax y su transpuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37510012 0.14510518 0.4797947 ]]\n",
      "\n",
      "[[0.37510012 0.         0.        ]\n",
      " [0.         0.14510518 0.        ]\n",
      " [0.         0.         0.4797947 ]]\n"
     ]
    }
   ],
   "source": [
    "# Primera matriz\n",
    "softmax_out = softmax(D3_out)\n",
    "print(softmax_out)\n",
    "print()\n",
    "print(np.diag(softmax_out.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1407001 , 0.05442897, 0.17997105],\n",
       "       [0.05442897, 0.02105551, 0.0696207 ],\n",
       "       [0.17997105, 0.0696207 , 0.23020296]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segunda matriz\n",
    "softmax_out.T.dot(softmax_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23440002 -0.05442897 -0.17997105]\n",
      " [-0.05442897  0.12404967 -0.0696207 ]\n",
      " [-0.17997105 -0.0696207   0.24959175]]\n"
     ]
    }
   ],
   "source": [
    "# Implemente la función\n",
    "print(softmax_jac(D3_out))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 7:\n",
    "¿Cuanto vale el jacobiano de la sofmax evaluado en la salida de D3?\n",
    "\n",
    "a) [[ 0.53440002 -0.05442897 -0.17997105]\n",
    " [-0.05442897  0.62404967 -0.0696207 ]\n",
    " [-0.17997105 -0.0696207   0.24959175]]\n",
    "b) [[ 0.83440002 -0.05442897 -0.17997105]\n",
    " [-0.05442897  0.92404967 -0.0696207 ]\n",
    " [-0.17997105 -0.0696207   0.24959175]]\n",
    "c) [[ 0.23440002 -0.05442897 -0.17997105]\n",
    " [-0.05442897  0.12404967 -0.0696207 ]\n",
    " [-0.17997105 -0.0696207   0.24959175]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular el error propagado hasta la salida de D3\n",
    "Tener en cuenta que si ya calculó el jacobiano de la softmax y el gradiente del MSE, lo unico que tiene que hacer es realizar un producto punto entre ambos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16048242]\n",
      " [ 0.01240618]\n",
      " [ 0.14807624]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_D3 = softmax_jac(D3_out).dot(MSE_grad_out)\n",
    "print(error_D3)\n",
    "error_D3.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 8\n",
    "¿Cuanto vale el error propagado hasta la salida de D3?\n",
    "\n",
    "(a) [-0.16048242  0.01240618  0.14807624]\n",
    "(b) [0.16048242  0.01240618  0.14807624]\n",
    "(c) [-0.16048242  -0.01240618  0.14807624]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de error propagado a la entrada de D3 o a la salida de A2\n",
    "Calcularlo en función de error_d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05109109]\n",
      " [-0.14390649]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_A2 = weights[4].dot(error_D3)\n",
    "print(error_A2)\n",
    "error_A2.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 9\n",
    "¿Cuanto vale el error propagado hasta la salida de A2 o la entrada de D3?\n",
    "\n",
    "(a) [0.05109109 -0.14390649]\n",
    "(b) [-0.05109109 -0.14390649]\n",
    "(c) [-0.05109109 0.14390649]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobiano de sigmoidea\n",
    "Para calcular el error propagado a la entrada de A2 es necesario calcular el jacobiano de la sigmoidea y evaluarlo en la entrada al bloque A2 o la salida de D2. Recordar que el resultado tiene que ser una matriz diagonal.\n",
    "\n",
    "La derivada de la función sigmoidea $\\sigma(x)$ es $\\sigma(x)(1- \\sigma(x))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23249061, 0.        ],\n",
       "       [0.        , 0.24667375]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_jac(D2_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 10:\n",
    "¿Cuanto vale el jacobiano de la sigmoidea evaluado en la salida de D2?\n",
    "\n",
    "(a) [ [-0.23249061, 0.        ], [0.        , 0.24667375] ]\n",
    "(b) [ [0.23249061, 0.        ], [-0.        , 0.24667375] ]\n",
    "(c) [ [0.23249061, 0.        ], [0.        , 0.24667375] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de error propagado a la entrada de A2 o a la salida de D2\n",
    "Calcularlo en función de error_A2 y el jacobiano de la sigmoidea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0118782 ]\n",
      " [-0.03549795]]\n"
     ]
    }
   ],
   "source": [
    "error_D2 = sigmoid_jac(D2_out).dot(error_A2)\n",
    "print(error_D2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 11:\n",
    "¿Cuanto vale el error propagado a la entrada de A2 o a la salida de D2?\n",
    "\n",
    "(a) [0.0118782  -0.03549795]\n",
    "(b) [-0.0118782  -0.03549795]\n",
    "(c) [-0.0118782  0.03549795]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo del error propagado a la salida de D1\n",
    "\n",
    "Ya tiene todos los elementos para calcular el error propagado a la salida de D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00380889]\n",
      " [-0.00120508]\n",
      " [-0.00164512]]\n"
     ]
    }
   ],
   "source": [
    "error_A1 = weights[2].dot(error_D2)\n",
    "error_D1 = sigmoid_jac(D1_out).dot(error_A1)\n",
    "print(error_D1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 12:\n",
    "¿Cuanto vale el error propagado a la salida de D1?\n",
    "\n",
    "(a) [ -0.00380889 -0.00120508 -0.00164512]\n",
    "(b) [ 0.00380889 0.00120508 0.00164512]\n",
    "(c) [ 0.00380889 -0.00120508 -0.00164512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo del gradiente de los pesos de D1\n",
    "Notar que es simplemente la multiplicación matricial entre el error acumulado (error_D1) y el jacobiano de la salida de D1 respecto a los pesos. Que puede verificar que no es otra cosa que la entrada a la red X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01295024 -0.00409727 -0.00559341]\n",
      " [ 0.00799867 -0.00253067 -0.00345476]]\n",
      "[[ 0.00380889 -0.00120508 -0.00164512]]\n"
     ]
    }
   ],
   "source": [
    "g_1_ws = np.matmul(error_D1, X).T\n",
    "print(g_1_ws)\n",
    "g_1_b = np.matmul(error_D1, np.array([[1]])).T\n",
    "print(g_1_b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 13:\n",
    "¿Cuanto vale el gradiente de los pesos y bias para D1?\n",
    "(a) w = [ [ 0.01295024 -0.00409727 -0.00559341],[ 0.00799867 -0.00253067 -0.00345476] ] b = [ 0.00380889 -0.00120508 -0.00164512]\n",
    "(b) w = [ [ 0.1295024 -0.00409727 -0.00559341],[ 0.00799867 -0.00253067 -0.00345476] ] b = [ 0.00380889 -0.00120508 -0.00164512]\n",
    "(c) w = [ [ 0.1295024 0.00409727 -0.0559341],[ 0.00799867 -0.00253067 -0.00345476] ] b = [ 0.00380889 -0.00120508 -0.00164512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo del gradiente de los pesos de D2\n",
    "Tener en cuenta que es solo multiplicar una matriz de error que ya tiene guardada con el vector de entrada a la capa D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00535625 -0.01600712]\n",
      " [-0.0108144  -0.03231878]\n",
      " [-0.00965336 -0.02884902]]\n",
      "[[-0.0118782  -0.03549795]]\n"
     ]
    }
   ],
   "source": [
    "g_2_ws = np.matmul(error_D2, A1_out).T\n",
    "print(g_2_ws)\n",
    "g_2_b = np.matmul(error_D2, np.array([[1]])).T\n",
    "print(g_2_b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 14:\n",
    "¿Cuanto vale el gradiente de los pesos y bias para D2?\n",
    "(a) w = [ [0.00535625 0.01600712], [-0.0108144  0.03231878], [-0.00965336 -0.02884902] ] b = [-0.0118782  -0.03549795]\n",
    "(b) w = [ [0.00535625 -0.01600712], [-0.0108144  0.03231878], [-0.00965336 -0.02884902] ] b = [-0.0118782  -0.03549795]\n",
    "(c) w = [ [-0.00535625 -0.01600712], [-0.0108144  -0.03231878], [-0.00965336 -0.02884902] ] b = [-0.0118782  -0.03549795]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo del gradiente de los pesos de D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05900569  0.00456147  0.05444422]\n",
      " [-0.08949681  0.0069186   0.08257822]]\n",
      "[[-0.16048242  0.01240618  0.14807624]]\n"
     ]
    }
   ],
   "source": [
    "g_3_ws = np.matmul(error_D3, A2_out).T\n",
    "print(g_3_ws)\n",
    "g_3_b = np.matmul(error_D3, np.array([[1]])).T\n",
    "print(g_3_b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 15:\n",
    "¿Cuanto vale el gradiente de los pesos y bias para D3?\n",
    "(a) w = [ [0.05900569  0.00456147  0.05444422], [-0.08949681  0.0069186   0.08257822] ] b = [-0.16048242  0.01240618  0.14807624]\n",
    "(b) w = [ [-0.05900569  0.00456147  0.05444422], [-0.08949681  0.0069186   0.08257822] ] b = [-0.16048242  0.01240618  0.14807624]\n",
    "(c) w = [ [0.05900569  0.00456147  -0.05444422], [-0.08949681  0.0069186   0.08257822] ] b = [-0.16048242  0.01240618  0.14807624]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "D1 (Dense)                   (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "A1 (Activation)              (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "D2 (Dense)                   (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "A2 (Activation)              (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "D3 (Dense)                   (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "P_est (Activation)           (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('red_softmax_lab01.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_gradient(capa):\n",
    "    inputs = tf.constant(X)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(inputs)\n",
    "        loss = model.loss(tf.constant(P_true), preds)\n",
    "\n",
    "    grads = tape.gradient(loss, model.get_layer(capa).trainable_variables)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[ 0.01295023, -0.00409727, -0.00559341],\n",
       "        [ 0.00799867, -0.00253067, -0.00345476]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.00380889, -0.00120508, -0.00164512], dtype=float32)>]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tf_gradient('D1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[-0.00535625, -0.01600713],\n",
       "        [-0.01081439, -0.03231878],\n",
       "        [-0.00965335, -0.02884902]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.0118782 , -0.03549796], dtype=float32)>]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tf_gradient('D2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[-0.05900569,  0.00456146,  0.05444422],\n",
       "        [-0.08949681,  0.0069186 ,  0.08257821]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.16048242,  0.01240617,  0.14807624], dtype=float32)>]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tf_gradient('D3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armar una función get_gradients(X, P_true, weights) que revuelva los gradienes de cada capa densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.01295024, -0.00409727, -0.00559341],\n",
      "       [ 0.00799867, -0.00253067, -0.00345476]]), array([[ 0.00380889, -0.00120508, -0.00164512]]), array([[-0.00535625, -0.01600712],\n",
      "       [-0.0108144 , -0.03231878],\n",
      "       [-0.00965336, -0.02884902]]), array([[-0.0118782 , -0.03549795]]), array([[-0.05900569,  0.00456147,  0.05444422],\n",
      "       [-0.08949681,  0.0069186 ,  0.08257822]]), array([[-0.16048242,  0.01240618,  0.14807624]]))\n",
      "0.2139194440951749\n"
     ]
    }
   ],
   "source": [
    "grads, loss = get_gradients(X, P_true, weights)\n",
    "print(grads)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2139194440951749\n",
      "0.20662613128588422\n",
      "0.19947171057325727\n",
      "0.1924681092888171\n",
      "0.18562604333803648\n",
      "0.1789549380634228\n",
      "0.17246287574065775\n",
      "0.16615656976223617\n",
      "0.16004136458874682\n",
      "0.1541212596879758\n",
      "0.1483989549782366\n",
      "0.1428759147709048\n",
      "0.1375524468781014\n",
      "0.1324277934098721\n",
      "0.12750022981379402\n",
      "0.12276716888239801\n",
      "0.11822526673855087\n",
      "0.1138705281725413\n",
      "0.10969840911474449\n",
      "0.10570391445541599\n",
      "0.10188168984418387\n",
      "0.09822610649742136\n",
      "0.0947313383986826\n",
      "0.09139143158766722\n",
      "0.0882003654930716\n",
      "0.08515210647403793\n",
      "0.08224065389624935\n",
      "0.0794600791863473\n",
      "0.07680455838766365\n",
      "0.07426839878711093\n",
      "0.07184606020334171\n",
      "0.06953217152552128\n",
      "0.06732154307530211\n",
      "0.06520917533624655\n",
      "0.06319026455874394\n",
      "0.0612602057074741\n",
      "0.05941459317511957\n",
      "0.05764921964221048\n",
      "0.05596007342010118\n",
      "0.054343334573122626\n",
      "0.05279537007760111\n",
      "0.051312728240080235\n",
      "0.04989213256493353\n",
      "0.04853047523265045\n",
      "0.04722481032435658\n",
      "0.04597234690544114\n",
      "0.04477044206132039\n",
      "0.04361659396113585\n",
      "0.04250843501033663\n",
      "0.0414437251403814\n",
      "0.040420345272985665\n"
     ]
    }
   ],
   "source": [
    "def get_updated_ws(weights, lr = 0.1):\n",
    "    grads, loss = get_gradients(X, P_true, weights)\n",
    "    new_w = []\n",
    "    for i, w in enumerate(weights):\n",
    "        new_w.append(w - lr*grads[i])\n",
    "    return new_w, loss\n",
    "\n",
    "new_w, loss = get_updated_ws(weights)\n",
    "print(loss)\n",
    "for i in range(50):\n",
    "    new_w, loss = get_updated_ws(new_w)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
