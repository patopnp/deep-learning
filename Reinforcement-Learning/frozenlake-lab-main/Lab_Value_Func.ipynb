{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc07cc05-1015-47fd-aeff-7a7a6f26a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a17abc-c190-4414-9c86-4d15e6b93625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861911d8-4a29-4b83-bf00-bbbfdd6be315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frozenlake_helper import get_frozenlake_env, policy_1, policy_2, policy_3, policy_4, LEFT, RIGHT, UP, DOWN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81f48fd-876d-4059-bc6e-861c0796284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [policy_1, policy_2, policy_3, policy_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a78871f-17c2-425f-ae7b-979e0d274d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f641b8-68a7-4003-ad41-a81db535d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_return(r, gamma=0.9):\n",
    "    # Por si es una lista\n",
    "    r = np.array(r, dtype=float)\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r[0]\n",
    "\n",
    "def run_episode(env, policy, start_pos, gamma=1.0):\n",
    "    obs = env.reset(start_pos)\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "    return get_discounted_return(rewards, gamma)\n",
    "\n",
    "def get_expected_return(env, policy, N=5000, start_pos=0, gamma=1.0):\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        reward = run_episode(env, policy, start_pos, gamma)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    rewards = np.array(rewards, dtype=float)\n",
    "    return rewards, np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5889c31-de8c-44e0-a893-9cc05c7e6ae5",
   "metadata": {},
   "source": [
    "# Ejercicio 1: Muestreo en entorno aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2547707-8b99-464e-beb7-bc21ab900be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_penalty = 0\n",
    "gamma = 1.0\n",
    "is_slippery = True\n",
    "env = get_frozenlake_env(is_slippery, step_penalty=step_penalty)\n",
    "policy = policy_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea79541-90a7-42d6-aa40-df5227002307",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(0)\n",
    "action = policy[1]\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463c9c0f-b9dd-4947-b58a-d5d9d7cf4546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prob': 0.3333333333333333}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1392934b-e770-4e91-80d2-a7b48794c7b3",
   "metadata": {},
   "source": [
    "### Armar una función que devuelva la estimación de la V(s)\n",
    "Recibe:\n",
    "- El entorno (env)\n",
    "- La política (policy)\n",
    "- La cantidad de episodios usados para la estimación\n",
    "\n",
    "Devuelve:\n",
    "- numpy array de longitud 12 con los \"value\" donde la posición indica el estado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d7aab5-8eb1-4f1c-8e86-76bd4629f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_V_sampling(env, policy, N=10_000, gamma=1.0):\n",
    "    Vs_sample = np.zeros(number_of_states)\n",
    "    for start_pos in range(number_of_states):\n",
    "        if start_pos in policy:\n",
    "            _, r_mean, r_std = get_expected_return(env, policy, N, start_pos, gamma)\n",
    "            Vs_sample[start_pos] = r_mean\n",
    "    return Vs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78a3c029-b29f-44ab-9d55-491211e83c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s) para policy 1\n",
      "[[0.807 1.    1.    1.   ]\n",
      " [0.586 1.    1.    1.   ]\n",
      " [0.    1.    1.    0.   ]]\n",
      "\n",
      "V(s) para policy 2\n",
      "[[0.545 0.652 0.785 0.855]\n",
      " [0.408 0.675 0.862 0.922]\n",
      " [0.    0.795 0.928 0.   ]]\n",
      "\n",
      "V(s) para policy 3\n",
      "[[0.604 0.724 0.83  0.855]\n",
      " [0.394 0.678 0.798 0.875]\n",
      " [0.    0.774 0.904 0.   ]]\n",
      "\n",
      "V(s) para policy 4\n",
      "[[0.    0.    0.    0.204]\n",
      " [0.    0.    0.    0.397]\n",
      " [0.    0.    0.    0.   ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 1_000\n",
    "for i, policy in enumerate(policies):\n",
    "    Vs_sample_policy_1 = estimate_V_sampling(env, policy, N=N, gamma=gamma)\n",
    "    print('V(s) para policy', i + 1)\n",
    "    print(Vs_sample_policy_1.reshape(3, 4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73ac5a-d82e-409e-9de2-a69b8a7fad3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0740818e-a11e-4055-812e-cb06ada664ee",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Armar modelos de entorno y recompenza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005b45c-4e3c-496f-aba1-4db145b9bb48",
   "metadata": {},
   "source": [
    "### Para el caso del entorno esto sería un diccionario: \n",
    "- con keys de todos los estados posibles (de 0 a 11)\n",
    "- para cada estado un diccionario con keys de las acciones posibles (LEFT, RIGHT, UP, DOWN)\n",
    "- para cada estado y acción un diccionario que indique el listado de las proximas acciones con sus probabilidades"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7aa4a5d3-8d56-4213-86db-f6159d252bc5",
   "metadata": {},
   "source": [
    "por ejemplo:\n",
    "\n",
    "print(transition_model)\n",
    "\n",
    "{0: {'LEFT': {0: 0.6672, 4: 0.3328},\n",
    "  'RIGHT': {4: 0.3333, 0: 0.331, 1: 0.3357},\n",
    "  'UP': {1: 0.3341, 0: 0.6659},\n",
    "  'DOWN': {0: 0.3296, 1: 0.338, 4: 0.3324}},\n",
    " 1: {'LEFT': {1: 0.3227, 0: 0.3362, 5: 0.3411},\n",
    "  'RIGHT': {5: 0.3314, 1: 0.3318, 2: 0.3368},\n",
    "  'UP': {2: 0.3338, 0: 0.3308, 1: 0.3354},\n",
    "  'DOWN': {2: 0.3292, 5: 0.3354, 0: 0.3354}},\n",
    "  \n",
    " ...\n",
    " \n",
    " 11: {'LEFT': {11: 1.0},\n",
    "  'RIGHT': {11: 1.0},\n",
    "  'UP': {11: 1.0},\n",
    "  'DOWN': {11: 1.0}}}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92783bb6-a5cf-494e-b9c4-677c1ff1b788",
   "metadata": {},
   "source": [
    "### Para el caso del modelo de recompenza sería: \n",
    "\n",
    "Igual al anterior solo que el ultimo diccionario contiene los proximos estados con la recompenza de cada uno.\n",
    "\n",
    "En este caso para simplificar el código se guarda:\n",
    "- count: cantidad de veces que entro en ese estado para calcular el reward\n",
    "- total_reward: la suma de los rewards\n",
    "- reward: El que nos interesa (total_reward/count)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f764aee-a6e9-4730-8268-b8dbaf67e4c5",
   "metadata": {},
   "source": [
    "por ejemplo:\n",
    "\n",
    "print(reward_model)\n",
    "{0: {'LEFT': {4: {'total_reward': 0, 'count': 3259, 'reward': 0.0},\n",
    "   0: {'total_reward': 0, 'count': 6741, 'reward': 0.0}},\n",
    "  'RIGHT': {4: {'total_reward': 0, 'count': 3329, 'reward': 0.0},\n",
    "   1: {'total_reward': 0, 'count': 3326, 'reward': 0.0},\n",
    "   0: {'total_reward': 0, 'count': 3345, 'reward': 0.0}},\n",
    "  'UP': {1: {'total_reward': 0, 'count': 3293, 'reward': 0.0},\n",
    "   0: {'total_reward': 0, 'count': 6707, 'reward': 0.0}},\n",
    "  'DOWN': {4: {'total_reward': 0, 'count': 3320, 'reward': 0.0},\n",
    "   1: {'total_reward': 0, 'count': 3391, 'reward': 0.0},\n",
    "   0: {'total_reward': 0, 'count': 3289, 'reward': 0.0}}},\n",
    " 1: {'LEFT': {0: {'total_reward': 0, 'count': 3351, 'reward': 0.0},\n",
    "   1: {'total_reward': 0, 'count': 3328, 'reward': 0.0},\n",
    "   5: {'total_reward': 0, 'count': 3321, 'reward': 0.0}},\n",
    "  'RIGHT': {5: {'total_reward': 0, 'count': 3270, 'reward': 0.0},\n",
    "   2: {'total_reward': 0, 'count': 3324, 'reward': 0.0},\n",
    "   1: {'total_reward': 0, 'count': 3406, 'reward': 0.0}},\n",
    "  'UP': {2: {'total_reward': 0, 'count': 3277, 'reward': 0.0},\n",
    "   0: {'total_reward': 0, 'count': 3380, 'reward': 0.0},\n",
    "   1: {'total_reward': 0, 'count': 3343, 'reward': 0.0}},\n",
    "  'DOWN': {2: {'total_reward': 0, 'count': 3335, 'reward': 0.0},\n",
    "   5: {'total_reward': 0, 'count': 3258, 'reward': 0.0},\n",
    "   0: {'total_reward': 0, 'count': 3407, 'reward': 0.0}}},\n",
    "   \n",
    " ...\n",
    " \n",
    " 11: {'LEFT': {11: {'total_reward': 0, 'count': 10000, 'reward': 0.0}},\n",
    "  'RIGHT': {11: {'total_reward': 0, 'count': 10000, 'reward': 0.0}},\n",
    "  'UP': {11: {'total_reward': 0, 'count': 10000, 'reward': 0.0}},\n",
    "  'DOWN': {11: {'total_reward': 0, 'count': 10000, 'reward': 0.0}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "882d5ba0-5ffe-46d2-b050-19ba346357a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_str = {\n",
    "    LEFT: 'LEFT',\n",
    "    RIGHT: 'RIGHT',\n",
    "    UP: 'UP',\n",
    "    DOWN: 'DOWN'\n",
    "}\n",
    "str_to_action = {\n",
    "    'LEFT': LEFT,\n",
    "    'RIGHT': RIGHT,\n",
    "    'UP': UP,\n",
    "    'DOWN': DOWN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b970d860-261a-4fe0-926b-7e0af1abd8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [LEFT, RIGHT, UP, DOWN]\n",
    "\n",
    "N = 10000\n",
    "transition_model = {}\n",
    "reward_model = {}\n",
    "\n",
    "for start_pos in range(number_of_states):\n",
    "    # Iteración en todos los estados\n",
    "    if start_pos not in transition_model:\n",
    "        # Inicializo diccionario del estado\n",
    "        transition_model[start_pos] = {}\n",
    "        reward_model[start_pos] = {}\n",
    "    for action in actions:\n",
    "        # Iteración en todas las acciones\n",
    "        action_str = action_to_str[action]\n",
    "        if action not in transition_model[start_pos]:\n",
    "            # inicializo diccinario de la accion\n",
    "            transition_model[start_pos][action_str] = {}\n",
    "            reward_model[start_pos][action_str] = {}\n",
    "        for n in range(N):\n",
    "            # Notar que no corro todo el episodio sino que solo me interesa la próxima acción\n",
    "            env.reset(start_pos)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            if obs not in transition_model[start_pos][action_str]:\n",
    "                # inicializo diccionario de proximo estado\n",
    "                transition_model[start_pos][action_str][obs] = 0\n",
    "                reward_model[start_pos][action_str][obs] = {}\n",
    "                reward_model[start_pos][action_str][obs]['total_reward'] = 0\n",
    "                reward_model[start_pos][action_str][obs]['count'] = 0\n",
    "                reward_model[start_pos][action_str][obs]['reward'] = 0\n",
    "\n",
    "            # Implementar \n",
    "            transition_model[start_pos][action_str][obs] += 1 \n",
    "            reward_model[start_pos][action_str][obs]['total_reward'] += reward\n",
    "            reward_model[start_pos][action_str][obs]['count'] += 1\n",
    "            reward_model[start_pos][action_str][obs]['reward'] = reward_model[start_pos][action_str][obs]['total_reward']/reward_model[start_pos][action_str][obs]['count']\n",
    "        \n",
    "# Normalización de modelo\n",
    "for state, actions in transition_model.items():\n",
    "    for action, next_state_count in actions.items():\n",
    "        total_count = 0\n",
    "        for next_state, count in next_state_count.items():\n",
    "            total_count = total_count + count\n",
    "        for next_state, count in next_state_count.items():\n",
    "            next_state_count[next_state] = count/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b91b3-cc67-4986-b351-951611c30c2c",
   "metadata": {},
   "source": [
    "### Depende el modelo del entorno de la policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d83d3b54-939b-4eb1-95f1-59bf3ea5d305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LEFT': {0: 0.6644, 4: 0.3356},\n",
       " 'RIGHT': {1: 0.3333, 4: 0.3319, 0: 0.3348},\n",
       " 'UP': {0: 0.662, 1: 0.338},\n",
       " 'DOWN': {1: 0.3345, 4: 0.3365, 0: 0.329}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd98604-1ac8-40a3-bad3-f0d4de8f261f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LEFT': {9: {'total_reward': 0, 'count': 3429, 'reward': 0.0},\n",
       "  10: {'total_reward': 0, 'count': 3252, 'reward': 0.0},\n",
       "  6: {'total_reward': 0, 'count': 3319, 'reward': 0.0}},\n",
       " 'RIGHT': {6: {'total_reward': 0, 'count': 3274, 'reward': 0.0},\n",
       "  11: {'total_reward': 3393.0, 'count': 3393, 'reward': 1.0},\n",
       "  10: {'total_reward': 0, 'count': 3333, 'reward': 0.0}},\n",
       " 'UP': {6: {'total_reward': 0, 'count': 3321, 'reward': 0.0},\n",
       "  11: {'total_reward': 3251.0, 'count': 3251, 'reward': 1.0},\n",
       "  9: {'total_reward': 0, 'count': 3428, 'reward': 0.0}},\n",
       " 'DOWN': {10: {'total_reward': 0, 'count': 3313, 'reward': 0.0},\n",
       "  9: {'total_reward': 0, 'count': 3264, 'reward': 0.0},\n",
       "  11: {'total_reward': 3423.0, 'count': 3423, 'reward': 1.0}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5c8e4d5c-08ba-4261-8973-c4e588e8fb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LEFT': {6: {'total_reward': 0, 'count': 3399, 'reward': 0.0},\n",
       "  3: {'total_reward': 0, 'count': 3271, 'reward': 0.0},\n",
       "  11: {'total_reward': 3330.0, 'count': 3330, 'reward': 1.0}},\n",
       " 'RIGHT': {11: {'total_reward': 3289.0, 'count': 3289, 'reward': 1.0},\n",
       "  7: {'total_reward': 0, 'count': 3344, 'reward': 0.0},\n",
       "  3: {'total_reward': 0, 'count': 3367, 'reward': 0.0}},\n",
       " 'UP': {6: {'total_reward': 0, 'count': 3286, 'reward': 0.0},\n",
       "  7: {'total_reward': 0, 'count': 3374, 'reward': 0.0},\n",
       "  3: {'total_reward': 0, 'count': 3340, 'reward': 0.0}},\n",
       " 'DOWN': {7: {'total_reward': 0, 'count': 3348, 'reward': 0.0},\n",
       "  6: {'total_reward': 0, 'count': 3387, 'reward': 0.0},\n",
       "  11: {'total_reward': 3265.0, 'count': 3265, 'reward': 1.0}}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ffd95-49c1-4238-a614-8113417810ef",
   "metadata": {},
   "source": [
    "# Ejercicio 3: Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "423cd6eb-db7f-4f9f-9ba0-d76ce5b881ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_V_by_value_iteration(policy, transition_model, reward_model, N=500, number_of_states=12):\n",
    "    Vs = np.zeros(number_of_states)\n",
    "    for i in range(N):\n",
    "        oldV = Vs.copy()\n",
    "        for s, v in enumerate(Vs):\n",
    "            if s in policy:\n",
    "                action = action_to_str[policy[s]]\n",
    "                avg_reward = 0\n",
    "                reward = 0\n",
    "                \n",
    "                for next_s, prob in transition_model[s][action].items():\n",
    "                    reward += prob * (oldV[next_s] + reward_model[s][action][next_s]['reward'])\n",
    "                \n",
    "                avg_reward = reward\n",
    "                Vs[s] = avg_reward\n",
    "    return Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc83bd-9bb0-4fc3-b9ef-c039154e298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def value_iteration(S, A, P, R):\n",
    "#    V = {s: 0 for s in S}\n",
    "    \n",
    "#    while True:\n",
    "#        oldV = V.copy()\n",
    "        \n",
    "#        for s in S:\n",
    "#            Q = {}\n",
    "#            for a in A:\n",
    "#                Q[a] = R(s, a) + sum(P(s_next, s, a) * oldV[s_next] for s_next in S)\n",
    "                \n",
    "#            V[s] = max(Q.values())\n",
    "            \n",
    "#        if all(oldV[s] == V[s] for s in S):\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7f17ce8a-0530-4f87-94d1-b5fa26a8a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "policy = policy_1\n",
    "Vs = estimate_V_by_value_iteration(policy, transition_model, reward_model, N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3154247d-5b7b-4af6-9c21-0267ae1d5b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8022516 , 0.99999786, 0.99999842, 0.9999986 ],\n",
       "       [0.6036721 , 0.99999814, 0.99999886, 0.99999931],\n",
       "       [0.        , 0.99999842, 0.99999931, 0.        ]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vs.reshape(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bacbd6ce-ba83-42f0-a29b-b0ca6a052c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79, 1.  , 1.  , 1.  ],\n",
       "       [0.55, 1.  , 1.  , 1.  ],\n",
       "       [0.  , 1.  , 1.  , 0.  ]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 200\n",
    "Vs_sample_policy = estimate_V_sampling(env, policy, N=N, gamma=gamma)\n",
    "Vs_sample_policy.reshape(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac80238-4edf-4d97-8d6c-a0362ae2a4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
